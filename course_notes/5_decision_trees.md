## Decision Trees
- Supervised Learning
  - Classification: Output is discrete.
  - Regression: Output is continuous.
- Classification Learning
  - Instances: input
  - Concept: function -> true / false
  - Target Concept: answer
  - Hypothesis class: all functions
  - Sample: training set
  - Candidate: concept == target concept
  - Testing set
- Decision Trees
  - Expressiveness: AND, OR, XOR
- ID3
  - Loop: 
    - A <- best attribute (max gain: p(v) log p(v))
    - Assign A as decision attribute
    - For each value of A, create a descendent of node
    - Sort training examples to leaves
    - If examples perfectly clssified, stop
    - Else, iterate over leavers
  - Bias:
    - Restriction Bias: H
    - Preference Bias: h âˆˆ H <-> Inductive Bias
      - Good Split At Top
      - Correct Over Incorrect
      - Shorter Trees 
- Other Considerations
  - Continuous Attribute
    - e.g. Age, Weight, Distance
  - Does it make sense to repeat an attribute along a path in the tree?
  - When do we stop?
    - Everything classified correctly!
    - No more attributes!
    - No overfitting
    - Pruning, Output: Vote
  - Regression
    - Splitting, Variance
    - Output: average, local, linear fit
- Entropy
  
  > ![equation](http://latex.codecogs.com/gif.latex?entropy=\sum_{i}-p_i\log_2(p_i))
  - Controls how a DT decides where to split the data
  - Definition: measure of impurity in a bunch of examples
- Information Gain
  - Information Gain = entropy (parent) - (weighted average) * entropy (children)
  - Decision tree algorithm: maximize information gain
